{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Transformar frases a vectores usando BERT","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyP85ElwK3TTPCFys1vQ25QB"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["# Installar transformers de HuggingFace\n","!pip install transformers"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6JbdB-QtSinz","executionInfo":{"status":"ok","timestamp":1661385467464,"user_tz":180,"elapsed":12188,"user":{"displayName":"Fernando Das Neves","userId":"06324247755041763216"}},"outputId":"7fcdf84a-c67f-4d11-f2d0-8c79dab323dc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.21.2-py3-none-any.whl (4.7 MB)\n","\u001b[K     |████████████████████████████████| 4.7 MB 5.3 MB/s \n","\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.12.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n","Collecting huggingface-hub<1.0,>=0.1.0\n","  Downloading huggingface_hub-0.9.0-py3-none-any.whl (120 kB)\n","\u001b[K     |████████████████████████████████| 120 kB 72.8 MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n","Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n","  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n","\u001b[K     |████████████████████████████████| 6.6 MB 41.8 MB/s \n","\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.1)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Installing collected packages: tokenizers, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.9.0 tokenizers-0.12.1 transformers-4.21.2\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CfmykF2uSCPR"},"outputs":[],"source":["import torch\n","from transformers import BertTokenizer, BertModel"]},{"cell_type":"code","source":["# modelo  pre-entrenado de BERT en español spanish a usar\n","# otros modelos que puede usar es 'dccuchile/bert-base-spanish-wwm-uncased'\n","bert_model = 'mrm8488/distill-bert-base-spanish-wwm-cased-finetuned-spa-squad2-es'\n","\n","# conseguir el tokenizer usado al entrenar al modelo de BERT \n","tokenizer = BertTokenizer.from_pretrained(bert_model)\n","\n","# Cargar un modelo pre-entrenado de BERT (los pesos de la NN)\n","model = BertModel.from_pretrained(bert_model,\n","            output_hidden_states = True, # Whether the model returns all hidden-states.\n",")\n","# Poner al modelo en modo  \"evaluation\", donde no va a entrenar, solo evaluar (solo feed-forward).\n","# Nota: Esto es algo de Pytorch, no de BERT\n","model.eval()"],"metadata":{"id":"ZOaPF3CZSJ_Y","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1661389247295,"user_tz":180,"elapsed":2228,"user":{"displayName":"Fernando Das Neves","userId":"06324247755041763216"}},"outputId":"955d0508-ec95-4936-be17-a1bc8ce7c81c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at mrm8488/distill-bert-base-spanish-wwm-cased-finetuned-spa-squad2-es were not used when initializing BertModel: ['qa_outputs.bias', 'qa_outputs.weight']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"execute_result","data":{"text/plain":["BertModel(\n","  (embeddings): BertEmbeddings(\n","    (word_embeddings): Embedding(31002, 768, padding_idx=1)\n","    (position_embeddings): Embedding(512, 768)\n","    (token_type_embeddings): Embedding(2, 768)\n","    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","    (dropout): Dropout(p=0.1, inplace=False)\n","  )\n","  (encoder): BertEncoder(\n","    (layer): ModuleList(\n","      (0): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          (intermediate_act_fn): GELUActivation()\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (1): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          (intermediate_act_fn): GELUActivation()\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (2): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          (intermediate_act_fn): GELUActivation()\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (3): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          (intermediate_act_fn): GELUActivation()\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (4): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          (intermediate_act_fn): GELUActivation()\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (5): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          (intermediate_act_fn): GELUActivation()\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (6): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          (intermediate_act_fn): GELUActivation()\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (7): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          (intermediate_act_fn): GELUActivation()\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (8): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          (intermediate_act_fn): GELUActivation()\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (9): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          (intermediate_act_fn): GELUActivation()\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (10): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          (intermediate_act_fn): GELUActivation()\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (11): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          (intermediate_act_fn): GELUActivation()\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","    )\n","  )\n","  (pooler): BertPooler(\n","    (dense): Linear(in_features=768, out_features=768, bias=True)\n","    (activation): Tanh()\n","  )\n",")"]},"metadata":{},"execution_count":26}]},{"cell_type":"code","source":["def bert_sentence_embedding(text:str) -> torch.Tensor:\n","    # Todas las frases en BERT comienzan con [CLS] y terminan con [SEP]\n","    marked_text = f\"[CLS] {text} [SEP]\"\n","\n","    # Tokenizar al texto usando el tokenizer de BERT.\n","    tokenized_text = tokenizer.tokenize(marked_text)\n","    \n","    # Mapear a los tokens a sus correspondienctes indices en el vocabulario del modelo\n","    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n","\n","    # Embeddings de segmentos: BERT puede (opcionalmente) usar pares de frases como entrada, especialmente para entrenar contestar preguntas.\n","    # No es lo que vamos a hacer, asi que le pasamos todos 1s\n","    segments_ids = [1] * len(tokenized_text)\n","\n","    # Convertir los indices de tokens a vectores de  PyTorch\n","    tokens_tensor = torch.tensor([indexed_tokens])\n","    segments_tensors = torch.tensor([segments_ids])\n","\n","\n","    # \"Pasar\" el texto a través de BERT, y guardar los estados de las 12 capas escondidas de la red\n","    with torch.no_grad():\n","        outputs = model(tokens_tensor, segments_tensors)\n","\n","        # Evaluating the model will return a different number of objects based on \n","        # how it's  configured in the `from_pretrained` call earlier. In this case, \n","        # becase we set `output_hidden_states = True`, the third item will be the \n","        # hidden states from all layers. See the documentation for more details:\n","        # https://huggingface.co/transformers/model_doc/bert.html#bertmodel\n","        hidden_states = outputs[2]\n","    \n","    # `token_vecs` es una matriz de tamaño  [nro_tokens x 768]\n","    token_vecs = hidden_states[-2][0]\n","\n","    # Calcular el promedio de todos los tokens.\n","    sentence_embedding = torch.mean(token_vecs, dim=0)\n","    return sentence_embedding"],"metadata":{"id":"0ChAIsBnS4Po"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["text = \"Quiero conseguir los embeddings de esta frase.\""],"metadata":{"id":"DXHRjgT-TCNH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sentence = bert_sentence_embedding(text)\n","# si necesitara convertir el vector de Torch a un vector de numpy:\n","sentence_np = sentence.numpy()\n"],"metadata":{"id":"Rnw0AflnTEdm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# conectar al disco en GDrive\n","from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eUM9yEazT0es","executionInfo":{"status":"ok","timestamp":1661385834270,"user_tz":180,"elapsed":25316,"user":{"displayName":"Fernando Das Neves","userId":"06324247755041763216"}},"outputId":"ffce4554-b993-4796-a970-31e36265f670"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["# /content/drive/MyDrive es la raíz de su \"disco\" en GDrive. \n","# Puede leer y escribir a Google Drive como si fuera un disco más\n","import os\n","os.listdir('/content/drive/MyDrive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xe2Bz-u8UDOb","executionInfo":{"status":"ok","timestamp":1661385884038,"user_tz":180,"elapsed":376,"user":{"displayName":"Fernando Das Neves","userId":"06324247755041763216"}},"outputId":"ac3e5daa-f3f4-4a67-e7d6-c851d61c8cd4"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['Entrepreneurs Guide to Customer Development.pdf',\n"," 'NoSQL',\n"," 'Statistics',\n"," 'NLP',\n"," 'Bargaining For Advantage - Summary.pdf',\n"," 'Rapidminer',\n"," 'Measuring API Usability.pdf',\n"," 'Imported from Google Notebook - My first notebook.gdoc',\n"," 'Copy of Simple SCRUM Agile Project Management.gsheet',\n"," 'Copy of Simple SCRUM Agile Project Management (version 2.1) (1).gsheet',\n"," 'Copy of Simple SCRUM Agile Project Management (version 2.1).gsheet',\n"," 'The Mythology of Big Data Presentation.pdf',\n"," 'RITA_en_Tecnopolis.mp4',\n"," 'text mining',\n"," 'Casamiento',\n"," 'videos viaje a europa',\n"," 'Google Fotos',\n"," 'Alvarez',\n"," 'Your big idea.gslides',\n"," 'examen-despegar',\n"," 'zendeskbot',\n"," 'Vacaciones',\n"," 'Ficha de propuesta de tema y Director del TF Maestría.docx.gdoc',\n"," 'papers final web mining',\n"," 'web mining',\n"," 'webcam-toy-photo1.jpg',\n"," 'webcam-toy-photo1.png',\n"," 'borrosa_oscura3.png',\n"," 'borrosa_oscura.png',\n"," 'borrosa_oscura2.png',\n"," 'borrosa_oscura4.png',\n"," 'buena.png',\n"," 'borrosa_oscura1.png',\n"," 'buena-contraste.png',\n"," 'webcam.py',\n"," 'Backup Your Mobile',\n"," 'django_app.zip',\n"," 'cheques-cmc7.tar.gz',\n"," 'Web Mining Final 2018.zip',\n"," 'Final 2018.tar.gz',\n"," 'Notas MDM 2018.ods',\n"," 'RESULTADOS DE BUSQUEDA DE COMPORTAMIENTO DE USUARIO QUE ES RIESGOSO PARA RIPIO.gdoc',\n"," 'Pronóstico Tecnológico AC 2018.gslides',\n"," 'Mi CV',\n"," 'MEDyGC PE-PROFESORES-TF-cargas.xlsx TRABAJOS FINALES.xlsx',\n"," 'MEDyGC PE-PROFESORES-TF-cargas.xlsx TRABAJOS FINALES.gsheet',\n"," 'Machine Learning y otras cosas.gslides',\n"," 'quiver',\n"," 'Formulario Alta Proveedor_ Profesores y otros.doc',\n"," 'andrea',\n"," 'CLTV Model.gsheet',\n"," \"Surely You're Joking, Mr. Feynman.zip\",\n"," 'CV_FADN - Profesional - ingles.odt',\n"," 'CV_FADN - Profesional - ingles.gdoc',\n"," 'Colab Notebooks',\n"," 'collab',\n"," 'Estimating CLTV.gdoc',\n"," 'text_mining_python.zip',\n"," 'Comparacion TPs.gdoc',\n"," 'Recibo  Hon . 28%_Das Neves.pdf',\n"," 'Recibo  Hon . 28%_Das Neves.gdoc',\n"," 'Documento sin título.gdoc',\n"," 'Web_mining_TP',\n"," 'TP Final.gdoc']"]},"metadata":{},"execution_count":15}]}]}