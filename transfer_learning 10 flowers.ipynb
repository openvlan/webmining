{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[],"mount_file_id":"1dm6Uchlbqrf2FdzhP5S-0CvpEuQAzUwW","authorship_tag":"ABX9TyOMvKoONVCs1Cg6QKOGE1G8"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"njEh8Cemo3zb","executionInfo":{"status":"ok","timestamp":1663952782173,"user_tz":180,"elapsed":6061,"user":{"displayName":"Fernando Das Neves","userId":"06324247755041763216"}},"outputId":"75ab655f-c9f8-490e-9c96-c95a5c948241"},"source":["# Antes de ejecutar: Activar GPUs como sigue:\n","# menu \"Entorno de Ejecucion\" -> \"Cambiar tipo de entorno de ejecucion\" -> \"Acelerador de Hardware\" = \"GPU\"\n","# How to save tensorflow model to google drive: https://stackoverflow.com/questions/67305778/how-to-save-tensorflow-model-to-google-drive\n","\n","import tensorflow as tf\n","print(\"Usandor Tensorflow version \" + tf.__version__)\n","\n","\n","if tf.test.gpu_device_name():\n","  print('Usando GPU: {}'.format(tf.test.gpu_device_name()))\n","else:\n","  print(\"Usando CPU.\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Usandor Tensorflow version 2.8.2\n","Usando GPU: /device:GPU:0\n"]}]},{"cell_type":"code","metadata":{"id":"IFNd6dlwv7T1"},"source":["##################################################################\n","# Este script carga VGG16, reemplaza la ultima capa de prediccion,\n","# y reentrena para clasificar imagenes de 10 categorias de flores.\n","##################################################################\n","\n","import h5py\n","from skimage.transform import resize\n","import numpy as np\n","\n","from keras.layers import Flatten, Dense, Dropout, Input, Conv2D, MaxPooling2D\n","from keras.models import Model, Sequential\n","from tensorflow.keras.optimizers import SGD\n","from keras.preprocessing.image import ImageDataGenerator\n","from keras.utils.vis_utils import plot_model\n","from keras.callbacks import EarlyStopping, ModelCheckpoint\n","from sklearn.model_selection import train_test_split\n","import sklearn.preprocessing\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fm-hHB0NyDux"},"source":["# algunos parametros del entrenamiento\n","batch_size = 32 # cada batch son 32 imagenes\n","epochs = 13 # entrenamos hasta 13 epochs (pasadas sobre el dataset de entrenamiento), a menos que paremos antes por early stopping\n","epochs_to_stop_after_no_improvement = 3 # cuantas epochs consecutivas no tienen que tener mejora para aplicar early stopping\n","num_cores = 4 # cambiar este numero a la cantidad de cores de su cpu\n","\n","# parametros del descenso de gradiente\n","learning_rate=0.001\n","learning_rate_decay=1e-6\n","learning_rate_momentum=0.7"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iI_H5qI_yF9w"},"source":["def preprocesar_imagen_como_caffe(image:np.ndarray) -> np.ndarray:\n","    \"\"\"\n","    Transforma las imagenes al formato con el que fue entrenado el modelo de VGG16 que estamos usando.\n","    :param image: Una imagen representada como una matriz de (largo en pixels, alto en pixels, 3 canales)\n","    :return La imagen transformada.\n","    \"\"\"\n","    # pasar imagen de  'RGB'->'BGR', porque el modelo ya entrenado de VGG16 que estamos usando proviene de Caffe, y fue entrenado en ese orden de channels\n","    image = image[:, :, ::-1]\n","    # central valor de los pixels alrededor del valor medio de cada canal en el conj. de entrenamiento,\n","    # esto se calcula simplemente promediando todos los valores de cada canal en todas las imagenes de entrenamiento en imagenet.\n","    image[:, :, 0] -= 103.939\n","    image[:, :, 1] -= 116.779\n","    image[:, :, 2] -= 123.68\n","    return image\n","\n","\n","def rescalar_imagenes(flower_images:np.ndarray) -> np.ndarray:\n","    \"\"\"\n","    Cambia el tama単o de las imagenes de flores al tama単o con el que esta entrenada VGG16: 224x224 pixels\n","    :param flower_images: Una matriz de (nro_imagenes, ancho en pixels, alto en pixels, 3 canales); cada valor de la matriz esta entre 0 y 255.\n","    :return: Otra matriz de las mismas dimensiones de 'flowers' pero con todos los valores entre 0 y 1.\n","    \"\"\"\n","    rescaled_images = np.empty((flower_images.shape[0], 224, 224, flower_images.shape[3]), dtype=flower_images.dtype)\n","    for i in range(0, flower_images.shape[0]):\n","        rescaled_images[i, 0:224, 0:224, ] = resize(flower_images[i] / 255.0, (224, 224), anti_aliasing=True) * 255.0\n","    return rescaled_images\n","\n","\n","def encode_onehot_labels(labels:np.ndarray) ->np.ndarray:\n","    \"\"\"\n","     Cambia la codificacion de las categorias de flores a one-hot encoding, que es lo que necesitamos para entrenar la NN.\n","     :param labels: Una lista o arreglo de strings, el i-avo string es la categoria de la i-ava imagen de entrenamiento.\n","     :return una matriz de tama単o (cantidad de ejemplos en labels, cantidad de categorias en 'labels') donde cada celda es 1 o 0. Hay 1 solo 1 por fila.\n","    \"\"\"\n","    label_binarizer = sklearn.preprocessing.LabelBinarizer()\n","    label_binarizer.fit(range(max(labels) + 1))\n","    return label_binarizer.transform(labels)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Emb12Xn9yLhM"},"source":["def VGG_16():\n","    \"\"\"\n","    Crea una red para clasificar imagenes con la arquitectura VGG16, para poder reusar los pesos de VGG16 entrenado con imagenet.\n","    \"\"\"\n","    img_input = Input(shape=(224,224,3)) # tama単o de imagenes es 224x224 y 3 canales de colores\n","\n","    # Block 1\n","    output = Conv2D(64, (3, 3),\n","                      activation='relu',\n","                      padding='same',\n","                      name='block1_conv1')(img_input)\n","    output = Conv2D(64, (3, 3),\n","                      activation='relu',\n","                      padding='same',\n","                      name='block1_conv2')(output)\n","    output = MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool')(output)\n","\n","    # Block 2\n","    output = Conv2D(128, (3, 3),\n","                      activation='relu',\n","                      padding='same',\n","                      name='block2_conv1')(output)\n","    output = Conv2D(128, (3, 3),\n","                      activation='relu',\n","                      padding='same',\n","                      name='block2_conv2')(output)\n","    output = MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool')(output)\n","\n","    # Block 3\n","    output = Conv2D(256, (3, 3),\n","                      activation='relu',\n","                      padding='same',\n","                      name='block3_conv1')(output)\n","    output = Conv2D(256, (3, 3),\n","                      activation='relu',\n","                      padding='same',\n","                      name='block3_conv2')(output)\n","    output = Conv2D(256, (3, 3),\n","                      activation='relu',\n","                      padding='same',\n","                      name='block3_conv3')(output)\n","    output = MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool')(output)\n","\n","    # Block 4\n","    output = Conv2D(512, (3, 3),\n","                      activation='relu',\n","                      padding='same',\n","                      name='block4_conv1')(output)\n","    output = Conv2D(512, (3, 3),\n","                      activation='relu',\n","                      padding='same',\n","                      name='block4_conv2')(output)\n","    output = Conv2D(512, (3, 3),\n","                      activation='relu',\n","                      padding='same',\n","                      name='block4_conv3')(output)\n","    output = MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool')(output)\n","\n","    # Block 5\n","    output = Conv2D(512, (3, 3),\n","                      activation='relu',\n","                      padding='same',\n","                      name='block5_conv1')(output)\n","    output = Conv2D(512, (3, 3),\n","                      activation='relu',\n","                      padding='same',\n","                      name='block5_conv2')(output)\n","    output = Conv2D(512, (3, 3),\n","                      activation='relu',\n","                      padding='same',\n","                      name='block5_conv3')(output)\n","    output = MaxPooling2D((2, 2), strides=(2, 2), name='block5_pool')(output)\n","\n","    # capa de clasificacion\n","    output = Flatten(name='flatten')(output)\n","    output = Dense(4096, activation='relu', name='fc1')(output)\n","    output = Dense(4096, activation='relu', name='fc2')(output)\n","    output = Dense(1000, activation='softmax', name='predictions')(output) # la salida son las 1000 categorias de imagenet\n","\n","    return Model(inputs=img_input, outputs=output, name='vgg16')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Tv7VlazpyORO"},"source":["# leer el dataset de 210 imagenes de flores junto con sus etiquetas\n","dataset = h5py.File(\"/content/drive/MyDrive/collab/transfer_learning/flowers_dataset/10FlowerColorImages.h5\",'r')\n","# El [()] indica que lea el dataset completo a memoria, en vez de leerlo bajo demanda\n","images = dataset['images'][()]\n","image_labels = dataset['labels'][()]\n","images = rescalar_imagenes(images)\n","onehot_labels = encode_onehot_labels(image_labels)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZXfOOjk9y557"},"source":["# armar la arquitectura de la red neuronal\n","pretrained_vgg16 = VGG_16()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AJdWYYMhy8NE"},"source":["# Leer los pesos de VGG16 ya entrenada con imagenet.\n","# El modelo fue entrenado con los canales de cada imagen en el orden 'BGR' que utiliza la biblitoeca \"Caffe\",\n","#  y c/pixel centrado sobre la media del dataset imagenet = [103.939, 116.779, 123.68]\n","# Las imagenes nuevas tienen que tener exactamente esta transformacion.\n","pretrained_vgg16.load_weights('/content/drive/MyDrive/collab/transfer_learning/imagenet_dataset/vgg16_weights.h5')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3096j1L7yVfx"},"source":["# setear a todas las capas, excepto la ultima de clasificacion \n","# como \"no entrenable\" (los pesos no se actualizaran).\n","# Si pone -2 en vez de -1, entonces va \"descongelar\" las ultimas 2 capas, y lo mismo con -3, -4...\n","for layer in pretrained_vgg16.layers[:-1]:\n","    layer.trainable = False\n","\n","# Descartamos la ultima capa de vgg16 y creamos una capa nueva, cuya entrada es la anteultima capa (\"fc2\") de vgg16.\n","# Necesitamos hacer esto porque el numero de categorias en el nuevo dataset es diferente al numero de categorias en imagenet.\n","new_layer = Dense(10, activation=\"softmax\", name=\"predict_10flowers\")(pretrained_vgg16.get_layer('fc2').output)\n","\n","# crear un nuevo modelo cuya salida es la nueva capa\n","new_model = Model(inputs=pretrained_vgg16.input, outputs=new_layer)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5q_Jonghyesi"},"source":["# compilar el modelo con SGD/momentum optimizer\n","# y un learning rate muuuuy lento\n","sgd = SGD(learning_rate=learning_rate, decay=learning_rate_decay, momentum=learning_rate_momentum, nesterov=True)\n","# regla en Keras:\n","# si loss=categorical_crossentropy => metrics=categorical_accuracy\n","# si loss=binary_crossentropy y mas de 2 clases => metrics=categorical_accuracy, sino metrics=binary_accuracy\n","new_model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['categorical_accuracy'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2iSzocdpyjei"},"source":["X_train, X_test, y_train, y_test = train_test_split(images, onehot_labels, train_size=0.7, test_size=0.3, shuffle=True, stratify=image_labels)\n","\n","# aumentar la cantidad de ejemplos de entrenamiento, inventando variaciones de las imagenes\n","train_datagen = ImageDataGenerator(\n","    width_shift_range = 0.1,\n","    height_shift_range=0.1,\n","    shear_range=0.2,\n","    zoom_range=[0.8, 1.4],\n","    horizontal_flip=True,\n","    rotation_range=10,\n","    preprocessing_function=preprocesar_imagen_como_caffe\n",")\n","\n","# testear con los casos de prueba sin rotar, cambiar tama単o ni nada, solo acomodados a la manera en que fue entrenada la NN\n","test_datagen = ImageDataGenerator(preprocessing_function=preprocesar_imagen_como_caffe)\n","\n","train_generator = train_datagen.flow(\n","    X_train,\n","    y_train,\n","    batch_size=batch_size,\n",")\n","\n","validation_generator = test_datagen.flow(\n","    X_test,\n","    y_test\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nFgMzQNoymQm"},"source":["# entrenar y guarda el mejor resultado\n","new_model.fit(\n","    train_generator,\n","    steps_per_epoch=len(y_train) // batch_size,\n","    epochs=epochs,\n","    validation_data=validation_generator,\n","    validation_steps=len(y_test) // batch_size,\n","    use_multiprocessing=True,\n","    workers=num_cores,\n","    # parar el entrenamiento si no mejora en 3 epochs consecutivas; guardar el mejor modelo hasta ese momento al final de cada epoch.\n","    # Aqui, la medida de \"mejor modelo\" es val_categorical_accuracy = accuracy en el dataset de test\n","    callbacks=[EarlyStopping(monitor='val_categorical_accuracy', patience=epochs_to_stop_after_no_improvement, verbose=1),\n","               ModelCheckpoint('/content/drive/MyDrive/collab/transfer_learning/vgg16_retrained_10flowers.h5', verbose=1, monitor='val_categorical_accuracy', save_best_only=True, mode='auto')]\n",")\n","\n","print(\"Entrenamiento finalizado.\")"],"execution_count":null,"outputs":[]}]}